{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Linguistic Data Science Week 1",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Baron-Sun/E-book-Research-/blob/master/Linguistic_Data_Science_Week_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5eM5FJY_MSFB"
      },
      "source": [
        "**Welcome to Linguistic Data Science!**\n",
        "\n",
        "The first thing we will be doing is a review of some of the basic parts of the Python language that we will be using. Right now we will look at built-in objects. Later, we will be using objects coming from the libraries `numpy`, `pandas`, and `nltk`. We will also cover how to use Jupyter notebooks (where we are now).\n",
        "\n",
        "You will be turning in your mini-project writeups in the form of Jupyter notebooks. Jupyter notebooks allow you to combine text, code, and visualization. They are an example of **literate code**: code that is meant to be read in the same way that text is meant to be read, and is accompanied by text and figures."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kh-Kp6NlMmiC"
      },
      "source": [
        "We will be dealing with text data. At first, we will work with small amounts of text data. As the course goes on, we work with large amounts of text data, and toward the end of the class, very large amounts of text data. \n",
        "\n",
        "For now, let's just look at a single paragraph worth of text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vbxs7CACuH7"
      },
      "source": [
        "text = \"\"\"It was the best of times, it was the worst of times, it was the age of\n",
        "wisdom, it was the age of foolishness, it was the epoch of belief, it was the \n",
        "epoch of incredulity, it was the season of Light, it was the season of Darkness,\n",
        "it was the spring of hope, it was the winter of despair, we had everything \n",
        "before us, we had nothing before us, we were all going direct to Heaven, we were\n",
        "all going direct the other way â€“ in short, the period was so far like the \n",
        "present period, that some of its noisiest authorities insisted on its being \n",
        "received, for good or for evil, in the superlative degree of comparison only.\n",
        "\"\"\"\n",
        "\n",
        "print(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LwhB2bQWhLlo"
      },
      "source": [
        "Write in some of your own text data below, any English text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ClZtKDaThTek"
      },
      "source": [
        "mytext1 = # TODO\n",
        "mytext2 = # TODO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i6k9VeAqN1Od"
      },
      "source": [
        "Here we've stored our data in an object of type `str`, named `text`.  In general, you can figure out the type of an object by calling `type` on it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1GF6cjC4Nqur"
      },
      "source": [
        "type(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2JKIhO1fSmgo"
      },
      "source": [
        "A `str` in Python 3 represents a string of Unicode characters. There is no problem using non-ASCII or non-English characters in a `str`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bke9yPibSnGG"
      },
      "source": [
        "chinese_text = \"è‡ªå¼·ä¸æ¯ï¼›åŽšå¾·è¼‰ç‰©\"\n",
        "print(chinese_text)\n",
        "\n",
        "emoji_text = \"this ðŸ§‘ is an emoji\"\n",
        "print(emoji_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9sTEpJSTsZX"
      },
      "source": [
        "String objects in Python are super versatile. You can figure out what you can do with an object by calling `help` on its type:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RIOsNv_OOPp8"
      },
      "source": [
        "help(str)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kf_qj_iJOdpV"
      },
      "source": [
        "You will use this a lot. No one has memorized everything there is to know about every type in any programming language. Expert programmers and data scientists usually know next to nothing about the programming languages and frameworks they work in, because they work in very many different languages and frameworks. What they are good at is looking up references, getting help fast, and playing around to quickly familiarize themselves with a system. Right now we are looking at basic Python types, but as we go on, we will be using various libraries for data analysis, visualization, and machine learning. It will be crucial to know how to get help on how to use these libraries. Sometimes, the `help` function will be enough. Other times, you will have to look at online documentation.\n",
        "\n",
        "Check out the help documentation for the type `str` above. Let's scroll through the documentation and look at some of the methods and try them out. For example, we see the method `upper`. Under the method name, it says `S.upper() -> str` which means that calling the method `upper` with zero arguments on the string `S` will return a new object of type `str`. Let's try it out:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "luWvwHHHJ7PB"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_NK4AZxOsGb"
      },
      "source": [
        "print(text.upper())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iC3mJEX6QJJ9"
      },
      "source": [
        "Now try out some other methods you see listed in the documentation. Try out the one that take zero arguments and ignore the ones with double underscores (`__`) for now. Try out three methods in the boxes below.\n",
        "\n",
        "You may get errors. Don't worry if you do. It's good to get an error, because it means you learned something about how the language works (or doesn't work). Just replace the code in the box and run it again until you get something that works."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gpqc3r3oP4aF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FgcnkWy2QzmE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMWyDE55Qzub"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Wiax5GTQ0WT"
      },
      "source": [
        "You can get the length of a `str` using the function `len`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bppp90ASSZXV"
      },
      "source": [
        "len(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9Z7eVVnUoe0"
      },
      "source": [
        "Strings can be added to each other, in which case they are concatenated. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-iuotDcSbqf"
      },
      "source": [
        "combined_text = chinese_text + emoji_text\n",
        "print(combined_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jrvdajvkfD09"
      },
      "source": [
        "So, if \"cat\" + \"cat\" = \"catcat\", then what is 2 * \"cat\"?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEpPrACkfLHP"
      },
      "source": [
        "\"cat\" + \"cat\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tuEH7tWHfNBM"
      },
      "source": [
        "2 * \"cat\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGLMNRI0U-Jk"
      },
      "source": [
        "You can index into `str`s as if they were arrays. For example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRVz5SWVUMGx"
      },
      "source": [
        "print(\"text[0]:\", text[0]) # Gives you the 0th character\n",
        "print(\"text[1]:\", text[1]) # Gives you the 1st character\n",
        "print(\"text[11]:\", text[11]) # Gives you the 11th character\n",
        "print(\"text[20:25]:\", text[5:10]) # Gives you characters 5 to 10 (non-inclusive)\n",
        "print(\"text[:5]:\", text[:5])  # Gives you the first 5 characters\n",
        "print(\"text[600:]:\", text[600:]) # Give you the 600th character up to the end\n",
        "print(\"text[-1]:\", text[-1]) # Gives you the last character\n",
        "print(\"text[-2]:\", text[-2]) # Gives you the second-to-last character"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89_1pO72WCR_"
      },
      "source": [
        "You can compare strings to each other using `<`, `>`, `<=` and `>=`. Can you figure out what this means? Try to figure it out using some of the cells below. (If you already know, then come up with a series of examples that demonstrate the answer.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9O4-rD_lV-s6"
      },
      "source": [
        "\"cat\" < \"dogs\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YSKwALKrV_e1"
      },
      "source": [
        "\"dog\" < \"cats\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QuGgVovYWK4q"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EnEJRlsEWM3T"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DBN4NHhWM9X"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKEdRYu3Wazx"
      },
      "source": [
        "Some of the `str` methods allow us to answer questions about the contents of the `str`. For example, check out the method `startswith`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDHU2PVmWNC9"
      },
      "source": [
        "text.startswith(\"It\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4PUyY-14XFvA"
      },
      "source": [
        "text.startswith(\"it\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfapzGtPWNXg"
      },
      "source": [
        "There's also `endswith`. Can you figure out why the following expression is `False`?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bF1xUuzidFd5"
      },
      "source": [
        "text.endswith(\"only.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qeWhp0DmdF5m"
      },
      "source": [
        "Try out `startswith` and `endswith` on one of your text strings. Find one example that is True and another example that is False."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "InFNRhH7XM5G"
      },
      "source": [
        "# True examples"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "puR5dTbqXNmv"
      },
      "source": [
        "# False examples"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cx0LCcNZXOAP"
      },
      "source": [
        "The methods `startswith` and `endswith` check if the beginning or ending of a string match some other string. How would you tell if a string contains another string *anywhere*---not just at the beginning or end? To do this, you can use this syntax:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wh2UinUMXmFd"
      },
      "source": [
        "\"times\" in text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwNCHf_kXn5O"
      },
      "source": [
        "\"Times\" in text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nTFY10KiYDLT"
      },
      "source": [
        "if \"é“\" in chinese_text:\n",
        "  print(\"It's there\")\n",
        "else:\n",
        "  print(\"It's not there\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdwSihdGYfO7"
      },
      "source": [
        "Another extremely useful method of `str` objects is `count`. `count` returns the number of times a substring appears in a string. For example, we can ask: How often does the word \"present\" show up in our English text?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-qn0O1AWXpJA"
      },
      "source": [
        "text.count(\"present\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7V1xxssZBdT"
      },
      "source": [
        "Now here's a question: how often does the *word* \"it\" show up in our `text`? Try to answer below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6uZ7eRjX4BR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qo1ZStdbZOUe"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nMXx24gDZOcv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VhAVNjp_ZOi9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCINwYwpdkQI"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0njQPiSZgLM"
      },
      "source": [
        "Often it will be useful to `split` a string into multiple parts. The method `split(x)` takes a string and breaks it up whenever it sees `x` inside the string. The result is a `list` of strings. For example:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bXPbTGzmZOnq"
      },
      "source": [
        "\"this is a string\".split(\" \")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OtOuOfvUcCLa"
      },
      "source": [
        "\"this is a string\".split(\"s\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPJvZN2ZZHGr"
      },
      "source": [
        "\"this is a string\".split(\"s \")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6M_OQv04dsaU"
      },
      "source": [
        "You might have noticed that our `text` contains multiple lines. Suppose we want to split it into eight strings, one representing each line. To do this, we need to know a bit about how lines are represented in text. In this text, and in a lot of the text data you will be dealing with, there is a special character called \"newline\", written `\\n`, which indicates a line break.  So in order to split the text blob into its lines, you could split on `\\n`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pcSi6Ky_eP-y"
      },
      "source": [
        "text.split(\"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kAWoS1Sk0CY"
      },
      "source": [
        "There's a subtlety here: in text formatted on a Mac or on any Unix-based systems, the line breaks are indicated by the special character `\\n`. But on text formatted on Windows machines, the line breaks are indicated by a sequence of two special characters, `\\r\\n`. (On other, more exotic operating systems, newlines might be indicated by other special characters or sequences of special characters.) This can get annoying, so Python provides a special method to split on newlines, which will do the right thing no matter what linebreak character is used:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNfpWNSSla93"
      },
      "source": [
        "text.splitlines()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAefwbyfcozz"
      },
      "source": [
        "Now we have data in a `list` of `str`s, and we will talk about `list`s. A `list` is a finite sequence of discrete elements. You can iterate through the elements of a `list` and do things with them. For example, the code below goes through each element in a list, makes it uppercase, then prints it out:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwE0eG9sc2Es"
      },
      "source": [
        "small_string = \"This is a string and this is also an object\"\n",
        "parts = small_string.split(\" \")\n",
        "for part in parts:\n",
        "  uppercase_part = part.upper()\n",
        "  print(uppercase_part)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FqLTtEXxcSVl"
      },
      "source": [
        "Now let's try counting the occurrences of the word \"this\" in `small_string` text using `split`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YudQkIlNa7MF"
      },
      "source": [
        "parts = small_string.split(\" \")\n",
        "num_this = 0\n",
        "for part in parts:\n",
        "  if part == \"this\":\n",
        "    num_this += 1\n",
        "    \n",
        "print(num_this)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KglTpEjXch2Y"
      },
      "source": [
        "Can you see a problem with this? Correct the problem below. There are multiple solutions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xr3QCfLycgRz"
      },
      "source": [
        "parts = small_string.split(\" \")\n",
        "num_this = 0\n",
        "for part in parts:\n",
        "  if part == \"this\":\n",
        "    num_this += 1\n",
        "    \n",
        "print(num_this)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZXqgfundAoA"
      },
      "source": [
        "Lists are also useful because you can index into them in the same way you index into strings. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1rwne74lebAA"
      },
      "source": [
        "print(parts[0])\n",
        "print(parts[1])\n",
        "print(parts[2])\n",
        "print(parts[:5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDiwMTMUelt2"
      },
      "source": [
        "You can also add them together just like strings,  use `in` on them like strings, and use `count`. You can find this out by calling `help(list)`!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nv4tvZZ6er1C"
      },
      "source": [
        "parts + parts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7y9msUiEfSSv"
      },
      "source": [
        "2 * parts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RhvxaYNgeuQ6"
      },
      "source": [
        "\"string\" in parts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bp3KKQQew94"
      },
      "source": [
        "\"is a string\" in parts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lb5qGKU-e7i2"
      },
      "source": [
        "parts.count(\"is\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEfnsuu0yp1w"
      },
      "source": [
        "Especially important are the methods that *mutate* a list by adding or removing elements. `append` adds an element:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5b7ERe3syz2T"
      },
      "source": [
        "my_list = ['this', 'is', 'a', 'list']\n",
        "my_list.append('of')\n",
        "my_list.append('items')\n",
        "print(my_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZgENyafOy_2v"
      },
      "source": [
        "The method `pop` returns the final element of a list and deletes it from the list."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zdiH7Vj1zDvX"
      },
      "source": [
        "last_thing = my_list.pop()\n",
        "print(last_thing)\n",
        "print(my_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4YbABa0rzJcY"
      },
      "source": [
        "You can delete an arbitrary list element by deleting its index using the keyword `del`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPWKHOtFzNTq"
      },
      "source": [
        "del my_list[1] \n",
        "print(my_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2f7fb2Qlf0nW"
      },
      "source": [
        "Look up the list methods with `help(list)` and try some of them out below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ese4oteeffv1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kl8AKAA0f4Pk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7nBI7WIf4Ry"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRKH7z8Wf48p"
      },
      "source": [
        "Now let's try to split up our Charles Dickens text into words. What are some problems here?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LrW_kQgf4Tu"
      },
      "source": [
        "words = text.split(\" \")\n",
        "print(words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B18rTVZQhYPx"
      },
      "source": [
        "Now I want to know: based on this `list` of words, how many times does the word \"it\" appear in the text? A naive way to do it might be:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44Uo4zf_hfwj"
      },
      "source": [
        "words.count(\"it\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vN8YOyBghhYF"
      },
      "source": [
        "But this is undercounting, for two reasons. Do you see why? Try to fix it below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZ2-0_PWhq_a"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uj88-URXh7F-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x63Vzqm6h7H-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCRH73Ljg6cI"
      },
      "source": [
        "The problem we're running into here is called **tokenization**: how do you take a string of text (a sequence of characters) and determine where are the boundaries between word tokens? This is the first real linguistic issue we are going to talk about. Because it turns out to be hard to say what counts as a word token in a text: in fact *there is no single solution that will work for all kinds of text*.  When you are analyzing text data, the first question you will have to ask is about tokenization.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bc1EYwINh9pU"
      },
      "source": [
        "The first and simplest trick for tokenization is to use Python's `split()` method with no arguments---this will magically split based on all whitespace characters, such as `\" \"`, `\"\\n\"`, \"`\\r\\n`\", and friends, plus it will also split only once if there are multiple whitespaces next to each other. So:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PaQSdrd7iWUR"
      },
      "source": [
        "weird_text = \"this  is\\tweird\\r\\n text as you can\\tsee\\n\"\n",
        "print(weird_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MsbfmA2tihIR"
      },
      "source": [
        "parts_of_weird_text = weird_text.split()\n",
        "print(parts_of_weird_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHrebbHzilkT"
      },
      "source": [
        "This nicely gets rid of a lot of weirdness and it helps us solve our problem with the Dickens text:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVGfqh5_ik6I"
      },
      "source": [
        "words = text.casefold().split()\n",
        "print(words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oBmViCOi4Ex"
      },
      "source": [
        "Now we can count the \"it\"s pretty easily:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "co0hKEPSi2id"
      },
      "source": [
        "words.count(\"it\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3buowvu2jAuS"
      },
      "source": [
        "But the word tokens are still a little off. Do you see why? Take a look at the content of the list `words` and see if the tokens are all corresponding to single words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qyO4dPWNi-gs"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jt6gRrWojR6r"
      },
      "source": [
        "The remaining problem is that many of the word tokens that come out this way have punctuation attached to them. If we are interested in the meaningful words in a text, this is a problem. We don't want to think of `incredulity,` and `incredulity` as two different words. If someone asked \"does this text contain the word incredulity?\" and you answer no, it contains the word `incredulity,`, then you will not be giving the right answer.\n",
        "\n",
        "One solution is to separate all the punctuation out into separate tokens. On the other hand, sometimes this may not be desirable: for example, it might make sense to keep the `.` attached to abbreviations like `Mr.`. Ultimately, your choice depends on what you will be doing with the word tokens later on.\n",
        "\n",
        "Tokenization is the first hard problem in linguistic data science. Now I'd like you to try to write a tokenizer: a function that takes in a string, and outputs a list of the *meaningful* word tokens. Your tokenizer should work for the Charles Dickens text and also for the following examples. It should produce output that *you* find satisfying.\n",
        "\n",
        "**Exercise**: Write a function that can take in `str` objects like `text` and the ones below, and which outputs a list of *meaningful* tokens. Each token should be one word or punctuation mark. No character from the text should be deleted (except whitespace). There are many ways to do it. Try to find a way that makes sense to you. Feel free to use any string and list methods you like. How many tokens do you end up with per test text?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qHInRD3xJFk4"
      },
      "source": [
        "test_text_1 = \"\"\"This is a sample text for tokenization, which demonstrates some\n",
        "of the problems that can come up. For example, compound-words can be tricky. And\n",
        "words like \"Mr.\" or like \"Ms. Molly's.\" You'll find that it isn't always clear \n",
        "what should count as a token and what shouldn't count. The truth is: there is \n",
        "not a single correct answer--it depends on what your analysis's goal is. \n",
        "You will have to make decisions about how to do it. (And later you may have to go\n",
        "back and change those decisions.) It's surprisingly tricky. \"\"\"\n",
        "\n",
        "test_text_2 = \"\"\"Sometimes different tokenization schemes will be required \n",
        "depending on the style of text you are dealing with (different styles): \n",
        "for example, :) :( ^_^ >:( #tokenizationistricky\"\"\"\n",
        "\n",
        "test_text_3 = \"\"\"In Rivendell Mr Frodo meets Gandalf, who explains why he didn't \n",
        "meet them at Bree as planned-- while imprisoned in Saruman's tower, he was able to \n",
        "escape with the aide of Gwaihir, a giant eagle. In the meantime, there are many \n",
        "meetings between various peoples, and Elrond calls a council to decide what \n",
        "should be done with the Ring.The Ring can only be destroyed by throwing it into \n",
        "the fires (that is, lava) of Mt. Doom, where it was forged. Mt. Doom is \n",
        "located in Mordor, near Sauron's fortress of Barad-dÃ»r, and will be an \n",
        "incredibly dangerous journeyâ€¦\"\"\" \n",
        "\n",
        "test_text_4 = \"\"\"Tokens are often categorized by character content or by context\n",
        "within the data stream. Categories are defined by the rules of the lexe. \n",
        "Categories often idnvolve grammar elements of the language used in the data \n",
        "stream. rogramming langsuages often categorize tokens as: identifiers, operators,\n",
        "grouping symbols, or by ata type. Written languages commonly categorize tokens \n",
        "as: nouns, verbs, adjectives, or punctuation. Categories are used for \n",
        "post-processing of the tokens either by the parser or by other functions in the \n",
        "program.\n",
        "\n",
        "A lexical analyzer generally does nothing with combinations of tokens,    task \n",
        "left for a parser. For example, a typical lexical analyzer recognizes \n",
        "parenthses as tokens, but does nothing to ensure that each \"(\" is matched \n",
        "with a \")\".\n",
        "\"\"\"\n",
        "\n",
        "test_text_5 = \"@JetBlue Nothing better than having a delayed flight #sarcasm I want to get home soon ðŸ˜¢ðŸ˜¢ðŸ˜¢\"\n",
        "\n",
        "test_text_6 = \"\"\"I finally found a place that carry ground bison meat and bison \n",
        "steak! They are currently having a sale on ground bison meat (90% meat and 10% \n",
        "fat) for $7.99!! Sprouts is selling the exact same meat to fat ratio for $15! I \n",
        "ended up nabbing 6 of them. The macro of the ground bison is 92 grams of protein\n",
        "and 44 grams of fat. The bison steak have ridiculously insane macros! I get 80 \n",
        "grams of protein and 6 grams of fat for $15. I only grabbed one since they are \n",
        "pricey and will be only for a special occasion.\n",
        "\n",
        "Aside from the bison meat, I enjoy going to whole food for their hot \n",
        "food bar/salad bar. The price is $8.99 per pound.\n",
        "\n",
        "The only thing I would complain about, is the rude people that shop at Whole \n",
        "Foods. This specific Whole Foods have a lot of traffic between 11am-2pm. Come \n",
        "before that or after and you're good!\"\"\"\n",
        "\n",
        "test_text_7 = \"\"\"I used to like this WF. But they haven't had Frozen Organic \n",
        "Broccoli for like three months! What in the world? Also they are constantly out \n",
        "of stock on Gluten free items such as Applegate products, and Glutino pretzels. \n",
        "It's really a hit or miss on the products which makes this store completely \n",
        "unreliable. On one occasion I had asked the butcher if they ever carried pasture\n",
        "raised pork and he said there is no such thing. ?? Also, the rotisserie chickens\n",
        "are awful! So dried out! Half of the chicken is not edible! Where is the quality\n",
        "control here? It's such a great location, I would expect much higher standard of\n",
        "quality.\"\"\"\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_Ekff1Ll7s2"
      },
      "source": [
        "def tokenize(s):\n",
        "  return s.split()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lypb1a6kmHVk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afqeZ9ChmSR8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Py1UvQ13ncbV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xyb6hzgemXW8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VEE08TIJoJVP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdFvOUfhLJvO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BH9Y6OtXLJ0T"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-3OEd_3LJ3q"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2YLh6VeLJ6D"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J9PRwj6kLJ8P"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1E0ME_m-0DYn"
      },
      "source": [
        "So, now (hopefully) we have a way to do tokenization that makes reasonable choices no matter the kind of text. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4hD1TzpER7N"
      },
      "source": [
        "### Off-the-shelf Tokenizers\n",
        "\n",
        "Tokenization is usually going to be the first step in your linguistic data analysis. It is unlikely you want to spend your time writing a custom tokenizer for every project. So here are some of the off-the-shelf tokenizers that are available. \n",
        "\n",
        "The most commonly used tokenizers come from the `nltk` library. Here is how you import `nltk` to use the Punkt Treebank tokenizer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lB7zQrecEhXd"
      },
      "source": [
        "import nltk\n",
        "nltk.download(\"punkt\")\n",
        "nltk.tokenize.word_tokenize(\"This is a test!!!!!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0B0yGbfzLA6h"
      },
      "source": [
        "There is also a \"casual\" tokenizer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kj2Ep2irKAmp"
      },
      "source": [
        "from nltk.tokenize import TweetTokenizer\n",
        "t = TweetTokenizer()\n",
        "t.tokenize(\"This is like a #tweet :)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feK-VqBBLZS-"
      },
      "source": [
        "Try to come up with some interesting text below that you think will get tokenized differently by these two different tokenizers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AbqA8D22LWIt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJBT51e5LWQ5"
      },
      "source": [
        "import random\n",
        "random.choices(\"abc\", weights=[.1, .1, .8])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGH5SR2sLWX4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OeEq8bv4LWbF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BoQw158VLU_l"
      },
      "source": [
        "### Counting Tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGT_WNGQ2e14"
      },
      "source": [
        "Typically the main thing we will do with tokens is count them. For example, asking how many times the word \"it\" shows up in a text. A way to characterize any text in a very general, very high-level way is to look at all the word *types* and ask how many times they appear in the text (as *tokens*). Python has an extremely useful builtin type called a `Counter` for doing this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LAptw3lR3Hk9"
      },
      "source": [
        "from collections import Counter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5MXK0Tp3JW5"
      },
      "source": [
        "tokens = tokenize(text)\n",
        "c = Counter(tokens)\n",
        "print(c)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uuTRXOUg3Swb"
      },
      "source": [
        "![alt text](https://)A `Counter` is a dictionary that holds count values. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GoPpGmew3X0K"
      },
      "source": [
        "print(c['the'])\n",
        "print(c['of'])\n",
        "print(c['epoch'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TavRHfA39d5"
      },
      "source": [
        "`Counter`s have some very useful magic methods. For example, you can add them together:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-FiUJiWO36iW"
      },
      "source": [
        "c2 = Counter(tokenize(emoji_text))\n",
        "c3 = c + c2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMgH_dpF4VYF"
      },
      "source": [
        "print(c3['epoch'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0juQXw04WgR"
      },
      "source": [
        "print(c3['ðŸ§‘'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q10bNcvS4bKs"
      },
      "source": [
        "If you query a `Counter` for the count of a word it has never seen, then it will return `0` by default:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXRDHhqCe1L3"
      },
      "source": [
        "c['antidisestablishmentarianism']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fk12yiNJe-MT"
      },
      "source": [
        "When you tokenize a text and throw it into a `Counter`, what you've created is a **bag of words**. \n",
        "\n",
        "A **bag of words** is the simplest way to represent text for data science, and it is always the first thing you should try. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L501bE_hck1o"
      },
      "source": [
        "[link text](https://)"
      ]
    }
  ]
}